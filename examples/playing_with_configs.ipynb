{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('software': conda)"
  },
  "interpreter": {
   "hash": "eb9d8ccfba993fe204e77cb7497041ea2864b7db215d14ecc31a7f8eeb5cd52c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Tutorial 2\n",
    "\n",
    "In this tutorial, we will see how to use the built-in function of Pyraug to set upd our own configuration for the trainer, models and samplers. This follows the section ``Setting up your own configuations`` of the documentation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Link between `.json` and `dataclasses`\n",
    "\n",
    "In pyraug, the configurations of the models, trainers and samplers are stored and used as dataclasses.dataclass and all inherit from the BaseConfig. Hence, any configuration class has a classmethod from_json_file coming from BaseConfig allowing to directly load config from `.json` files into dataclasses or save dataclasses into a ``.json`` file."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Loading a configuration from a `.json`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Since all `ModelConfig` inherit from `BaseModelConfig` data class, any pyraug's model configuration can be loaded from a `.json` file with the `from_json_file` classmethod. Defining your own `model_config.json` may be useful when you decide to use the Pyraug's scripts which take as arguments paths to json files.\n",
    "\n",
    "**note:** Make sure the keys and types match the one expected in the `dataclass` or errors will be raised. Check documentation to find the expected types and keys "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "BaseModelConfig(input_dim=784, latent_dim=10, uses_default_encoder=True, uses_default_decoder=True)\n"
     ]
    }
   ],
   "source": [
    "from pyraug.models.base.base_config import BaseModelConfig\n",
    "config = BaseModelConfig.from_json_file('_demo_data/configs/model_config.json')\n",
    "print(config)"
   ]
  },
  {
   "source": [
    "Let's try with a `RHVAE` model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RHVAEConfig(input_dim=784, latent_dim=10, uses_default_encoder=True, uses_default_decoder=True, n_lf=3, eps_lf=0.0001, beta_zero=0.3, temperature=1.5, regularization=0.01, uses_default_metric=True)\n"
     ]
    }
   ],
   "source": [
    "from pyraug.models.rhvae import RHVAEConfig\n",
    "config = RHVAEConfig.from_json_file('_demo_data/configs/rhvae_config.json')\n",
    "print(config)"
   ]
  },
  {
   "source": [
    "### Saving a configuration to a `.json`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Conversely, you can save a `dataclass` quite easily using the `save_json` method coming form `BaseModelConfig`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "BaseModelConfig(input_dim=None, latent_dim=11, uses_default_encoder=True, uses_default_decoder=True)\n"
     ]
    }
   ],
   "source": [
    "from pyraug.models.base.base_config import BaseModelConfig\n",
    "\n",
    "my_model_config = BaseModelConfig(latent_dim=11)\n",
    "print(my_model_config)"
   ]
  },
  {
   "source": [
    "Save the `.json` file ..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model_config.save_json(dir_path='_demo_data/configs', filename='my_model_config')"
   ]
  },
  {
   "source": [
    "... and reload it "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "BaseModelConfig(input_dim=None, latent_dim=11, uses_default_encoder=True, uses_default_decoder=True)"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "BaseModelConfig.from_json_file('_demo_data/configs/my_model_config.json')"
   ]
  },
  {
   "source": [
    "The same can be done with a `TrainingConfig` or `SamplerConfig`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TrainingConfig(output_dir=None, batch_size=50, max_epochs=10, learning_rate=0.1, train_early_stopping=50, eval_early_stopping=None, steps_saving=None, seed=8, no_cuda=False, verbose=True)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TrainingConfig(output_dir=None, batch_size=50, max_epochs=10, learning_rate=0.1, train_early_stopping=50, eval_early_stopping=None, steps_saving=None, seed=8, no_cuda=False, verbose=True)"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "from pyraug.trainers.training_config import TrainingConfig\n",
    "my_training_config = TrainingConfig(max_epochs=10, learning_rate=0.1)\n",
    "print(my_training_config)\n",
    "my_training_config.save_json(dir_path='_demo_data/configs', filename='my_training_config')\n",
    "TrainingConfig.from_json_file('_demo_data/configs/my_training_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "BaseSamplerConfig(output_dir=None, batch_size=10, samples_per_save=100, no_cuda=False)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "BaseSamplerConfig(output_dir=None, batch_size=10, samples_per_save=100, no_cuda=False)"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "from pyraug.models.base.base_config import BaseSamplerConfig\n",
    "my_sampler_config = BaseSamplerConfig(batch_size=10, samples_per_save=100)\n",
    "print(my_sampler_config)\n",
    "my_sampler_config.save_json(dir_path='_demo_data/configs', filename='my_sampler_config')\n",
    "BaseSamplerConfig.from_json_file('_demo_data/configs/my_sampler_config.json')"
   ]
  },
  {
   "source": [
    "## Setting up configs in `Pipelines`\n",
    "\n",
    "Let's consider the example of Tutorial 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([200, 28, 28])"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "mnist_trainset = datasets.MNIST(root='../data', train=True, download=True, transform=None)\n",
    "n_samples = 200\n",
    "dataset_to_augment = mnist_trainset.data[:n_samples] \n",
    "dataset_to_augment.shape"
   ]
  },
  {
   "source": [
    "### Amending the model parameters\n",
    "\n",
    "Conversely to tutorial 1, we here first instantiate a model we want to train to avoid using the default on. Ths `Model` instance will then be passed to the `TrainingPipeline` for training. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Let's set up a custom model config and build the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyraug.models.rhvae import RHVAEConfig\n",
    "\n",
    "model_config = RHVAEConfig(\n",
    "    input_dim=28*28, # This is needed since we do not provide any encoder, decoder and metric architecture\n",
    "    latent_dim=9,\n",
    "    eps_lf=0.0001,\n",
    "    temperature=0.9\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(9,\n",
       " 0.0001,\n",
       " Parameter containing:\n",
       " tensor([0.9000]))"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "from pyraug.models import RHVAE\n",
    "\n",
    "model = RHVAE(\n",
    "    model_config=model_config\n",
    ")\n",
    "model.latent_dim, model.eps_lf, model.temperature"
   ]
  },
  {
   "source": [
    "### Amending training parameters\n",
    "\n",
    "In the meantime we can also amend the training parameter through the `TrainingConfig` instance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TrainingConfig(output_dir='my_model_with_custom_parameters', batch_size=200, max_epochs=20, learning_rate=0.001, train_early_stopping=100, eval_early_stopping=None, steps_saving=None, seed=8, no_cuda=False, verbose=True)"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "from pyraug.trainers.training_config import TrainingConfig\n",
    "\n",
    "training_config = TrainingConfig(\n",
    "    output_dir='my_model_with_custom_parameters',\n",
    "    no_cuda=False,\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=200,\n",
    "    train_early_stopping=100,\n",
    "    steps_saving=None,\n",
    "    max_epochs=5)\n",
    "training_config"
   ]
  },
  {
   "source": [
    "Now we only have to pass the model and the training config to the TrainingPipeline to perform training !"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyraug.pipelines import TrainingPipeline\n",
    "\n",
    "torch.manual_seed(8)\n",
    "pipeline = TrainingPipeline(\n",
    "    data_loader=None,\n",
    "    data_processor=None,\n",
    "    model=model,\n",
    "    optimizer=None,\n",
    "    training_config=training_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Data normalized using individual_min_max_scaling.\n",
      " -> If this is not the desired behavior pass an instance of DataProcess with 'data_normalization_type' attribute set to desired normalization or None\n",
      "Model passed sanity check !\n",
      "\n",
      "Created my_model_with_custom_parameters/training_2021-06-29_20-22-52. \n",
      "Training config, checkpoints and final model will be saved here.\n",
      "\n",
      "Successfully launched training !\n",
      "----------------------------------\n",
      "Training ended!\n",
      "Saved final model in my_model_with_custom_parameters/training_2021-06-29_20-22-52/final_model\n"
     ]
    }
   ],
   "source": [
    "pipeline(\n",
    "    train_data=dataset_to_augment,\n",
    "    log_output_dir='output_logs'\n",
    ")"
   ]
  },
  {
   "source": [
    "Now, the model and training parameters are saved in `json` files in `my_model_with_custom_parameters/training_YYYY-MM-DD_hh-mm-ss/final_model` and we can reload any of them."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_training = sorted(os.listdir('my_model_with_custom_parameters'))[-1]"
   ]
  },
  {
   "source": [
    "Let's get the saved `Trainingconfig` ..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TrainingConfig(output_dir='my_model_with_custom_parameters', batch_size=200, max_epochs=20, learning_rate=0.001, train_early_stopping=100, eval_early_stopping=None, steps_saving=None, seed=8, no_cuda=False, verbose=True)"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "TrainingConfig.from_json_file(os.path.join('my_model_with_custom_parameters', last_training, 'final_model/training_config.json'))"
   ]
  },
  {
   "source": [
    "... and rebuild the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rec = RHVAE.load_from_folder(os.path.join('my_model_with_custom_parameters', last_training, 'final_model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(9,\n",
       " 0.0001,\n",
       " Parameter containing:\n",
       " tensor([0.9000]))"
      ]
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "source": [
    "model_rec.latent_dim, model_rec.eps_lf, model_rec.temperature"
   ]
  },
  {
   "source": [
    "### Amending the Sampler parameters\n",
    "\n",
    "Of course, we can also amend the sampler parameters that is used within the `GenerationPipeline` as well. Again, simpy, build a `ModelSampler` instance and pass it to the `GenerationPipeline`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyraug.models.rhvae import RHVAESamplerConfig\n",
    "\n",
    "sampler_config = RHVAESamplerConfig(\n",
    "        output_dir='my_generated_data_with_custom_parameters',\n",
    "        mcmc_steps_nbr=100,\n",
    "        batch_size=100,\n",
    "        n_lf=5,\n",
    "        eps_lf=0.01\n",
    "        )"
   ]
  },
  {
   "source": [
    "Build the sampler"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyraug.models.rhvae.rhvae_sampler import RHVAESampler\n",
    "\n",
    "sampler = RHVAESampler(model=model_rec, sampler_config=sampler_config)"
   ]
  },
  {
   "source": [
    "At initialization, the sampler creates the folder where the generated data should be saved in case it does not exist."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Now we only have to pass the model and the sampler to the GenerationPipeline to perform generation !"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyraug.pipelines import GenerationPipeline\n",
    "\n",
    "generation_pipe = GenerationPipeline(\n",
    "    model=model_rec,\n",
    "    sampler=sampler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Created my_generated_data_with_custom_parameters/generation_2021-06-29_20-23-04.Generated data and sampler config will be saved here.\n",
      "\n",
      "Generation successfully launched !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generation_pipe(5)"
   ]
  },
  {
   "source": [
    "Now, the sampler parameters are saved in a `json` file in `my_generated_data_with_custom_parameters/training_YYYY-MM-DD_hh-mm-ss/final_model` and we can reload any it to check everything is ok ."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_generation = sorted(os.listdir('my_generated_data_with_custom_parameters'))[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RHVAESamplerConfig(output_dir='my_generated_data_with_custom_parameters', batch_size=100, samples_per_save=500, no_cuda=False, mcmc_steps_nbr=100, n_lf=5, eps_lf=0.01, beta_zero=1.0)"
      ]
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "RHVAESamplerConfig.from_json_file(os.path.join('my_generated_data_with_custom_parameters', last_generation, 'sampler_config.json' ))"
   ]
  }
 ]
}
{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('pyraug': conda)"
  },
  "interpreter": {
   "hash": "f88350844cc66b6a411660360f42b2428ad88920b1387de64253931efe6ecc81"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tutorial 2\n",
    "\n",
    "In this tutorial, we will see how to use the built-in function of Pyraug to set upd our own configuration for the trainer, models and samplers. This follows the section ``Setting up your own configuations`` of the documentation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Link between `.json` and `dataclasses`\n",
    "\n",
    "In pyraug, the configurations of the models, trainers and samplers are stored and used as dataclasses.dataclass and all inherit from the BaseConfig. Hence, any configuration class has a classmethod from_json_file coming from BaseConfig allowing to directly load config from `.json` files into dataclasses or save dataclasses into a ``.json`` file."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading a configuration from a `.json`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since all `ModelConfig` inherit from `BaseModelConfig` data class, any pyraug's model configuration can be loaded from a `.json` file with the `from_json_file` classmethod. Defining your own `model_config.json` may be useful when you decide to use the Pyraug's scripts which take as arguments paths to json files.\n",
    "\n",
    "**note:** Make sure the keys and types match the one expected in the `dataclass` or errors will be raised. Check documentation to find the expected types and keys "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# If you run this notebook on colab uncomment the following lines\n",
    "#!pip install pyraug\n",
    "#!git clone https://github.com/clementchadebec/pyraug.git\n",
    "#import os\n",
    "#path=os.path.join(os.getcwd(), 'pyraug/examples')\n",
    "#os.chdir(path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from pyraug.models.base.base_config import BaseModelConfig\n",
    "config = BaseModelConfig.from_json_file('_demo_data/configs/model_config.json')\n",
    "print(config)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "BaseModelConfig(input_dim=784, latent_dim=10, uses_default_encoder=True, uses_default_decoder=True)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's try with a `RHVAE` model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from pyraug.models.rhvae import RHVAEConfig\n",
    "config = RHVAEConfig.from_json_file('_demo_data/configs/rhvae_config.json')\n",
    "print(config)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RHVAEConfig(input_dim=784, latent_dim=10, uses_default_encoder=True, uses_default_decoder=True, n_lf=3, eps_lf=0.0001, beta_zero=0.3, temperature=1.5, regularization=0.01, uses_default_metric=True)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Saving a configuration to a `.json`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Conversely, you can save a `dataclass` quite easily using the `save_json` method coming form `BaseModelConfig`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from pyraug.models.base.base_config import BaseModelConfig\n",
    "\n",
    "my_model_config = BaseModelConfig(latent_dim=11)\n",
    "print(my_model_config)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "BaseModelConfig(input_dim=None, latent_dim=11, uses_default_encoder=True, uses_default_decoder=True)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save the `.json` file ..."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "my_model_config.save_json(dir_path='_demo_data/configs', filename='my_model_config')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "... and reload it "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "BaseModelConfig.from_json_file('_demo_data/configs/my_model_config.json')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "BaseModelConfig(input_dim=None, latent_dim=11, uses_default_encoder=True, uses_default_decoder=True)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The same can be done with a `TrainingConfig` or `SamplerConfig`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from pyraug.trainers.training_config import TrainingConfig\n",
    "my_training_config = TrainingConfig(max_epochs=10, learning_rate=0.1)\n",
    "print(my_training_config)\n",
    "my_training_config.save_json(dir_path='_demo_data/configs', filename='my_training_config')\n",
    "TrainingConfig.from_json_file('_demo_data/configs/my_training_config.json')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TrainingConfig(output_dir=None, batch_size=50, max_epochs=10, learning_rate=0.1, train_early_stopping=50, eval_early_stopping=None, steps_saving=None, seed=8, no_cuda=False, verbose=True)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TrainingConfig(output_dir=None, batch_size=50, max_epochs=10, learning_rate=0.1, train_early_stopping=50, eval_early_stopping=None, steps_saving=None, seed=8, no_cuda=False, verbose=True)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "from pyraug.models.base.base_config import BaseSamplerConfig\n",
    "my_sampler_config = BaseSamplerConfig(batch_size=10, samples_per_save=100)\n",
    "print(my_sampler_config)\n",
    "my_sampler_config.save_json(dir_path='_demo_data/configs', filename='my_sampler_config')\n",
    "BaseSamplerConfig.from_json_file('_demo_data/configs/my_sampler_config.json')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "BaseSamplerConfig(output_dir=None, batch_size=10, samples_per_save=100, no_cuda=False)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "BaseSamplerConfig(output_dir=None, batch_size=10, samples_per_save=100, no_cuda=False)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setting up configs in `Pipelines`\n",
    "\n",
    "Let's consider the example of Tutorial 1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "mnist_trainset = datasets.MNIST(root='../data', train=True, download=True, transform=None)\n",
    "n_samples = 200\n",
    "dataset_to_augment = mnist_trainset.data[:n_samples] \n",
    "dataset_to_augment.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([200, 28, 28])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Amending the model parameters\n",
    "\n",
    "Conversely to tutorial 1, we here first instantiate a model we want to train to avoid using the default on. Ths `Model` instance will then be passed to the `TrainingPipeline` for training. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's set up a custom model config and build the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from pyraug.models.rhvae import RHVAEConfig\n",
    "\n",
    "model_config = RHVAEConfig(\n",
    "    input_dim=28*28, # This is needed since we do not provide any encoder, decoder and metric architecture\n",
    "    latent_dim=9,\n",
    "    eps_lf=0.0001,\n",
    "    temperature=0.9\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "from pyraug.models import RHVAE\n",
    "\n",
    "model = RHVAE(\n",
    "    model_config=model_config\n",
    ")\n",
    "model.latent_dim, model.eps_lf, model.temperature"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(9,\n",
       " 0.0001,\n",
       " Parameter containing:\n",
       " tensor([0.9000]))"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Amending training parameters\n",
    "\n",
    "In the meantime we can also amend the training parameter through the `TrainingConfig` instance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "from pyraug.trainers.training_config import TrainingConfig\n",
    "\n",
    "training_config = TrainingConfig(\n",
    "    output_dir='my_model_with_custom_parameters',\n",
    "    no_cuda=False,\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=200,\n",
    "    train_early_stopping=100,\n",
    "    steps_saving=None,\n",
    "    max_epochs=5)\n",
    "training_config"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TrainingConfig(output_dir='my_model_with_custom_parameters', batch_size=200, max_epochs=5, learning_rate=0.001, train_early_stopping=100, eval_early_stopping=None, steps_saving=None, seed=8, no_cuda=False, verbose=True)"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we only have to pass the model and the training config to the TrainingPipeline to perform training !"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "from pyraug.pipelines import TrainingPipeline\n",
    "\n",
    "torch.manual_seed(8)\n",
    "pipeline = TrainingPipeline(\n",
    "    data_loader=None,\n",
    "    data_processor=None,\n",
    "    model=model,\n",
    "    optimizer=None,\n",
    "    training_config=training_config)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "pipeline(\n",
    "    train_data=dataset_to_augment,\n",
    "    log_output_dir='output_logs'\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Data normalized using individual_min_max_scaling.\n",
      " -> If this is not the desired behavior pass an instance of DataProcess with 'data_normalization_type' attribute set to desired normalization or None\n",
      "\n",
      "Model passed sanity check !\n",
      "\n",
      "Created my_model_with_custom_parameters/training_2021-09-03_09-23-26. \n",
      "Training config, checkpoints and final model will be saved here.\n",
      "\n",
      "Successfully launched training !\n",
      "----------------------------------\n",
      "Training ended!\n",
      "Saved final model in my_model_with_custom_parameters/training_2021-09-03_09-23-26/final_model\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, the model and training parameters are saved in `json` files in `my_model_with_custom_parameters/training_YYYY-MM-DD_hh-mm-ss/final_model` and we can reload any of them."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "last_training = sorted(os.listdir('my_model_with_custom_parameters'))[-1]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's get the saved `Trainingconfig` ..."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "TrainingConfig.from_json_file(os.path.join('my_model_with_custom_parameters', last_training, 'final_model/training_config.json'))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TrainingConfig(output_dir='my_model_with_custom_parameters', batch_size=200, max_epochs=5, learning_rate=0.001, train_early_stopping=100, eval_early_stopping=None, steps_saving=None, seed=8, no_cuda=False, verbose=True)"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "... and rebuild the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "model_rec = RHVAE.load_from_folder(os.path.join('my_model_with_custom_parameters', last_training, 'final_model'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "model_rec.latent_dim, model_rec.eps_lf, model_rec.temperature"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(9,\n",
       " 0.0001,\n",
       " Parameter containing:\n",
       " tensor([0.9000]))"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Amending the Sampler parameters\n",
    "\n",
    "Of course, we can also amend the sampler parameters that is used within the `GenerationPipeline` as well. Again, simpy, build a `ModelSampler` instance and pass it to the `GenerationPipeline`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "from pyraug.models.rhvae import RHVAESamplerConfig\n",
    "\n",
    "sampler_config = RHVAESamplerConfig(\n",
    "        output_dir='my_generated_data_with_custom_parameters',\n",
    "        mcmc_steps_nbr=100,\n",
    "        batch_size=100,\n",
    "        n_lf=5,\n",
    "        eps_lf=0.01\n",
    "        )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Build the sampler"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "from pyraug.models.rhvae.rhvae_sampler import RHVAESampler\n",
    "\n",
    "sampler = RHVAESampler(model=model_rec, sampler_config=sampler_config)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "At initialization, the sampler creates the folder where the generated data should be saved in case it does not exist."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we only have to pass the model and the sampler to the GenerationPipeline to perform generation !"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "from pyraug.pipelines import GenerationPipeline\n",
    "\n",
    "generation_pipe = GenerationPipeline(\n",
    "    model=model_rec,\n",
    "    sampler=sampler\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "generation_pipe(5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Created my_generated_data_with_custom_parameters/generation_2021-09-03_09-23-27.Generated data and sampler config will be saved here.\n",
      "\n",
      "Generation successfully launched !\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, the sampler parameters are saved in a `json` file in `my_generated_data_with_custom_parameters/training_YYYY-MM-DD_hh-mm-ss/final_model` and we can reload any it to check everything is ok ."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "last_generation = sorted(os.listdir('my_generated_data_with_custom_parameters'))[-1]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "RHVAESamplerConfig.from_json_file(os.path.join('my_generated_data_with_custom_parameters', last_generation, 'sampler_config.json' ))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RHVAESamplerConfig(output_dir='my_generated_data_with_custom_parameters', batch_size=100, samples_per_save=500, no_cuda=False, mcmc_steps_nbr=100, n_lf=5, eps_lf=0.01, beta_zero=1.0)"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {}
  }
 ]
}